scala> val numbers = List(1,2,3,4,5)
numbers: List[Int] = List(1, 2, 3, 4, 5)

scala> val myrdd = sc.parallelize(numbers)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26
scala> myrdd.count()
scala> val namesRdd = sc.parallelize(Seq("joe","john","adom"))
namesRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:24

scala> val namesRdd = sc.parallelize(Seq("joe","john","adom"),2)
namesRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:24

scala> namesRdd.foreach(println)

scala> val data = sc.textFile("demo.csv")
scala> sc.makeRDD(1 to 100,3)


a->a*a
>>>val x = sc.parallelize(List("spark", "rdd", "example",  "sample", "example"), 3)
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> val y = x.map(e => (e,1))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:29

scala> y.collect
res0: Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))

scala> val y = x.map(e => (1,e))
y: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[2] at map at <console>:29

scala> y.collect
res1: Array[(Int, String)] = Array((1,spark), (1,rdd), (1,example), (1,sample), (1,example))

scala> val y = x.map(e => (e,e.length))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:29

scala> y.collect
res2: Array[(String, Int)] = Array((spark,5), (rdd,3), (example,7), (sample,6), (example,7))

scala> val y = x.map(e => e.length)
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[4] at map at <console>:29

scala> y.collect
res3: Array[Int] = Array(5, 3, 7, 6, 7)


a->a*a
>>>val x = sc.parallelize(List("spark", "rdd", "example",  "sample", "example"), 3)
x: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> val y = x.map(e => (e,1))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[1] at map at <console>:29

scala> y.collect
res0: Array[(String, Int)] = Array((spark,1), (rdd,1), (example,1), (sample,1), (example,1))

scala> val y = x.map(e => (1,e))
y: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[2] at map at <console>:29

scala> y.collect
res1: Array[(Int, String)] = Array((1,spark), (1,rdd), (1,example), (1,sample), (1,example))

scala> val y = x.map(e => (e,e.length))
y: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:29

scala> y.collect
res2: Array[(String, Int)] = Array((spark,5), (rdd,3), (example,7), (sample,6), (example,7))

scala> val y = x.map(e => e.length)
y: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[4] at map at <console>:29

scala> y.collect
res3: Array[Int] = Array(5, 3, 7, 6, 7)

----------------------------------------------------------------------------------------------------

word count scala



val text = sc.textFile("input.txt") 
 val counts = text.flatMap(line => line.split(" ")
 ).map(word => (word,1)).reduceByKey(_+_) counts.collect


